{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Local PTHA WaaS documentation Here is the documentation concerning the local Probabilistic Tsunami Hazard Analysis (PTHA) Workflow as a Service (WaaS). The workflow aims at computing PTHA at a local scale for a given target site, starting from a regional hazard model, presently NEAMTHM18 , that is the long-term hazard model for the NEAM region (North-East Atlantic, Mediterranean and connected seas). One target site is allowed for the moment, future upgrades will allow to process more than one. The execution, graphically represented by a block diagram , is subdivided into 7 sequential steps, which are briefly described here . The tsunami simulations are carried out exploiting the numerical GPU code Tsunami-HySEA. The present version requires the entire execution of the workflow on the same HPC cluster equipped with GPUs; future versions will allow to send the simulations to a remote cluster (equipped with GPUs), while executing all the other steps on a local cluster. Info Some background details about local PTHA can be found here . Warning Presently, the use of the following clusters is implemented: - mercalli @INGV (job manager: PBS) - leonardo @CINECA (job manager: SLURM) More clusters could be added in the future. Content Installation Instructions Usage Instructions Workflow steps","title":"Home"},{"location":"#welcome-to-local-ptha-waas-documentation","text":"Here is the documentation concerning the local Probabilistic Tsunami Hazard Analysis (PTHA) Workflow as a Service (WaaS). The workflow aims at computing PTHA at a local scale for a given target site, starting from a regional hazard model, presently NEAMTHM18 , that is the long-term hazard model for the NEAM region (North-East Atlantic, Mediterranean and connected seas). One target site is allowed for the moment, future upgrades will allow to process more than one. The execution, graphically represented by a block diagram , is subdivided into 7 sequential steps, which are briefly described here . The tsunami simulations are carried out exploiting the numerical GPU code Tsunami-HySEA. The present version requires the entire execution of the workflow on the same HPC cluster equipped with GPUs; future versions will allow to send the simulations to a remote cluster (equipped with GPUs), while executing all the other steps on a local cluster. Info Some background details about local PTHA can be found here . Warning Presently, the use of the following clusters is implemented: - mercalli @INGV (job manager: PBS) - leonardo @CINECA (job manager: SLURM) More clusters could be added in the future.","title":"Welcome to Local PTHA WaaS documentation"},{"location":"#content","text":"Installation Instructions Usage Instructions Workflow steps","title":"Content"},{"location":"background/l-ptha/","text":"Local-scale PTHA Probabilistic Tsunami Hazard Analysis (PTHA) quantifies the likelihood of exceeding a specified measure of tsunami inundation at a given location within a given time interval. It provides scientific guidance for decision making regarding coastal engineering and evacuation planning. PTHA considers a discretization of the total hazard into many potential scenarios together with an evaluation of the probability of each scenario. Site-specific (i.e. local) PTHA, with an adequate discretization of source scenarios, combined with high-resolution inundation modeling, requires tens to hundreds of thousands of moderately intensive numerical simulations. In recent years, more efficient GPU-based High-Performance Computing (HPC) facilities, together with efficient GPU-optimized shallow water type models for simulating tsunami inundation, have made regional and local long-term hazard assessment feasible. The present tool allows for assessing local PTHA at a given target site by exploiting the NEAM Tsunami Hazard Model 2018 or NEAMTHM18 ( Basili et al., 2021 ) as a starting point. In principle, different regional models could be used, although this capability is not implemented yet. A description of the PTHA procedure applied to the town of Catania on the Eastern coast of Sicily can be found in Gibbons et al. (2020) .","title":"Local-scale PTHA"},{"location":"background/l-ptha/#local-scale-ptha","text":"Probabilistic Tsunami Hazard Analysis (PTHA) quantifies the likelihood of exceeding a specified measure of tsunami inundation at a given location within a given time interval. It provides scientific guidance for decision making regarding coastal engineering and evacuation planning. PTHA considers a discretization of the total hazard into many potential scenarios together with an evaluation of the probability of each scenario. Site-specific (i.e. local) PTHA, with an adequate discretization of source scenarios, combined with high-resolution inundation modeling, requires tens to hundreds of thousands of moderately intensive numerical simulations. In recent years, more efficient GPU-based High-Performance Computing (HPC) facilities, together with efficient GPU-optimized shallow water type models for simulating tsunami inundation, have made regional and local long-term hazard assessment feasible. The present tool allows for assessing local PTHA at a given target site by exploiting the NEAM Tsunami Hazard Model 2018 or NEAMTHM18 ( Basili et al., 2021 ) as a starting point. In principle, different regional models could be used, although this capability is not implemented yet. A description of the PTHA procedure applied to the town of Catania on the Eastern coast of Sicily can be found in Gibbons et al. (2020) .","title":"Local-scale PTHA"},{"location":"background/neamthm18/","text":"The regional hazard model NEAMTHM18 Basili et al., 2021","title":"The regional hazard model NEAMTHM18"},{"location":"background/neamthm18/#the-regional-hazard-model-neamthm18","text":"Basili et al., 2021","title":"The regional hazard model NEAMTHM18"},{"location":"background/thysea/","text":"The Tsunami-HySEA code","title":"The Tsunami-HySEA code"},{"location":"background/thysea/#the-tsunami-hysea-code","text":"","title":"The Tsunami-HySEA code"},{"location":"instructions/how-to-download/","text":"How to download","title":"How to download"},{"location":"instructions/how-to-download/#how-to-download","text":"","title":"How to download"},{"location":"instructions/installation/","text":"Installation The present version of the workflow requires the entire execution is carried out on the same (HPC) cluster, which must be equipped with GPUs in order to be able to run the tsunami simulations. Here is a table of the presently available clusters: INSTITUTE CLUSTER INGV mercalli CINECA leonardo The hosting institute will provide instructions on how to connect to the cluster. Note Before installing, check that the software and data requirements are satisfied. Then, follow these instructions: download the package from the master branch of the gitLab repository using the \"Code\" button on the top-right of the repository homepage and choosing the preferred compression format; Warning The gitLab repository is private and can be accessed only by members. If you are not a member, please contact us . connect to the cluster and transfer the package to the preferred folder (hereinafter referred to as /path-to-software/ ); unpack the archive: a folder cheese-ptha-master will be created, whose structure is illustrated in this diagram ; create/choose a working folder for IO (hereinafter referred to as /path-to-wdir/ ) where all the outputs will be saved; copy and rename the provided template for input files to your prefererred location, for example in your working folder (but other locations can be chosen as well): cp /path-to-software/cheese-ptha-master/templates/workflow_input.json.template /path-to-wdir/workflow_input.json cp /path-to-software/cheese-ptha-master/templates/load_env.source.template /path-to-wdir/load_env.source Instructions on how to fill out the JSON file are provided on the dedicated page , while the load_env.source file should be modified according to the preferred package and environment management system. We suggest Spack to build and configure the software environment, but a different package manager (e.g. conda) can be also used. unpack the initial conditions for PS seismicity (see NEAMTHM18 for details) by typing: cd /path-to-software/cheese-ptha-master/inputs/SUBDUCTIONS_from_NEAMTHM18 bunzip2 INIT_COND_PS.tar.bz2 tar -xvf INIT_COND_PS.tar This operation can take a while and can be skipped at the beginning, but must be executed before running STEP 4 of the workflow. create/choose a folder for the topo-bathimetric grid files ( /path-to-grids/ ). Having this folder, and the grid files inside it, is not mandatory until the tsunami simulations are executed ( STEP 5 ).","title":"Installation"},{"location":"instructions/installation/#installation","text":"The present version of the workflow requires the entire execution is carried out on the same (HPC) cluster, which must be equipped with GPUs in order to be able to run the tsunami simulations. Here is a table of the presently available clusters: INSTITUTE CLUSTER INGV mercalli CINECA leonardo The hosting institute will provide instructions on how to connect to the cluster. Note Before installing, check that the software and data requirements are satisfied. Then, follow these instructions: download the package from the master branch of the gitLab repository using the \"Code\" button on the top-right of the repository homepage and choosing the preferred compression format; Warning The gitLab repository is private and can be accessed only by members. If you are not a member, please contact us . connect to the cluster and transfer the package to the preferred folder (hereinafter referred to as /path-to-software/ ); unpack the archive: a folder cheese-ptha-master will be created, whose structure is illustrated in this diagram ; create/choose a working folder for IO (hereinafter referred to as /path-to-wdir/ ) where all the outputs will be saved; copy and rename the provided template for input files to your prefererred location, for example in your working folder (but other locations can be chosen as well): cp /path-to-software/cheese-ptha-master/templates/workflow_input.json.template /path-to-wdir/workflow_input.json cp /path-to-software/cheese-ptha-master/templates/load_env.source.template /path-to-wdir/load_env.source Instructions on how to fill out the JSON file are provided on the dedicated page , while the load_env.source file should be modified according to the preferred package and environment management system. We suggest Spack to build and configure the software environment, but a different package manager (e.g. conda) can be also used. unpack the initial conditions for PS seismicity (see NEAMTHM18 for details) by typing: cd /path-to-software/cheese-ptha-master/inputs/SUBDUCTIONS_from_NEAMTHM18 bunzip2 INIT_COND_PS.tar.bz2 tar -xvf INIT_COND_PS.tar This operation can take a while and can be skipped at the beginning, but must be executed before running STEP 4 of the workflow. create/choose a folder for the topo-bathimetric grid files ( /path-to-grids/ ). Having this folder, and the grid files inside it, is not mandatory until the tsunami simulations are executed ( STEP 5 ).","title":"Installation"},{"location":"instructions/json_input/","text":"The JSON input file JSON (JavaScript Object Notation) is an open standard file format that uses human-readable text. The data are written in key/value pairs, separated by a colon(:). Different key/value pairs are separated by a comma(,). A template of the workflow_input.json is provided, where all the required keys are already defined and grouped as objects, i.e. collections of key/value pairs surrounded by a curly brace. Comments are disseminated all along the file for the sake of clarity, and featured by the key \"_comment\". Before running the workflow, the JSON input file should be appropriately filled, although some sections may be neglected, depending on which steps will be executed. In other words, not all the sections in the JSON file need to be filled from the beginning if the corresponding step is not expected to be executed, as each step can be turned on/off by setting True/False within the input file. On the other hand, each step needs the output of the previous steps. As explained in the installation instructions , the template must be copied and renamed. All the fields will now be described. SETTINGS This first section is mandatory from the very beginning. The (complete) paths for the working folder and the folders where the workflow and the topo-bathymetric grids are located must be defined. In addition, the script (with path) to activate the software environment must also be declared here, to run the steps on the computational nodes of the HPC cluster. Example { \"work_path\" : \"/work/user/ptha/test\" , \"workflow_dir\" : \"/home/user/cheese-ptha-master\" , \"bathy_dir\" : \"/work/user/ptha/bathy\" , \"env_file\" : \"/work/user/ptha/load_env.source\" } STEPS Each workflow step can be turned on/off by setting them as True/False , depending on whether they must be executed. It is worth noting that the output of each step is generally an input for the next one (except for STEP 4 , whose output is required by STEP 6 , while STEP 5 needs STEP 3 ). For example, only STEP 2 would be executed with the following setup, provided that STEP 1 has been previously done. Example { \"step1\" : False , \"step2\" : True , \"step3\" : False , \"step4\" : False , \"step5\" : False , \"step6\" : False , \"step7\" : False } POI_SELECTION Given the target site, the closest Points of Interest (POIs) are selected from the regional hazard model NEAMTHM18 in STEP 1 , and then used in the analysis. First of all, the geographic area of the target site must be declared, choosing among Mediterranean Sea (\"med\"), North-East Atlantic Ocean (\"nea\"), or Black Sea (\"black\"). It is worth noting that the workflow is automatized only for target sites overlooking the Mediterranean Sea, while some manual operations are required when dealing with the other regions. Then, in the Mediterranean Sea three different strategies are possible to select the POI(s), prioritized as follows: 1. if the high-resolution grid for the target site is available (and stored in the declared folder bathy_dir ) its domain can be used to draw a rectangular region containing the coastline of interest, and the POIs included in that region are selected; 2. if the coordinates of the target site are given, they are used to build the rectangle and search for the POIs within it; 3. if none of the previous options are provided, the label(s) of the POI(s) from NEAMTHM18 must be directly indicated, as in the following example: Example { \"domain\" : \"med\" , \"grdfile\" : \" \" , \"lonlat\" : \" \" , \"poinames\" : \"med07706,med07712\" } The last option is mandatory outside the Mediterranean Sea. REGIONAL MODEL The default regional model is NEAMTHM18 , and the related files are expected to be stored in a folder named \"TSUMAPS1.x\" depending on the version. Version 1.0 must be used for managing sites outside the Mediterranean Sea, while for the Mediterranean area an updated vesion 1.1 is available. The path in the example is referred to the cluster mercalli @INGV. Example { \"folderMain\" : \"/scratch/projects/cat/SPTHA/\" , \"project_name\" : \"TSUMAPS1.1\" } DISAGGREGATION When a disaggregation procedure is required to select relevant scenarios for the target site, some parameters must be defined such as: the choice between a disaggregation on MIH (Maximum Inundation Height) intervals (i) or thresholds (t); in case of intervals, the MIH interval(s), expressed in m, where the disaggregation must be executed (it can be a single interval, e.g. 1-4, or a sequence of intervals, e.g. 1-4,4-8); in case of thresholds, the MIH value(s), expressed in m, where the disaggregation must be executed (it can be one or more values separated by comma, e.g. 1,4); the desired percentage of hazard reproduction for each interval/threshold. For example, the following settings will select the scenarios by disaggregating the hazard to account for 90% of the total at 1 m and 4 m: Example { \"disaggregation\" : True , \"type\" : \"t\" , \"mih_int\" : \" \" , \"mih_val\" : \"1,4\" , \"threshold\" : \"90\" } SAMPLING An alternative/complementary method for scenario selection is the importance sampling . Warning The importance sampling has not been implemented yet, so it must be set as False { \"sampling\": False } SIMULATION SETUP Some parameters for the tsunami simulations must be defined, such as: the number of levels of nested grids; an array defining the refinement ratio of the nesting from the coarsest to the finest grid (it is worth noting that with the simulation code Tsunami-HySEA , a refinement ratio equal to a power of 2 is mandatory); the simulation length expressed in hours. For example, if the simulations are carried out for 4 hours on a grid setup made of an outer grid spanning a regional domain at a resolution of 320 m, and 4 local-scale telescopic grids with increasing resolution equal to 160, 80, 20, and 5 m respectively, this section would be: Example { \"nlevels\" : 5 , \"ref_ratio\" : [ 2 , 2 , 4 , 4 ], \"propagation\" : 4 } HAZARD The hazard aggregation is implemented both in Python and MATLAB, and the user can choose the preferred language, also depending on the software installed on the cluster where the workflow is running. Moreover, this step is parallelized on the domain, through a horizontal decomposition of the highest resolution grid in \"slices\", which are then processed in parallel. The number of slices (i.e. the number of parallel processes) is computed at runtime, but the maximum allowed number must be declared here, as a trade-off between parallelism and manageability. In addition, the desired forecast time window has to be set, i.e the time period for which the probability of exceedance must be computed (a typical value is 50 years). Finally, a logical variable allows to possibly write the hazard curves as a CSV table. For example: Example { \"haz_code\" : \"python\" , \"nmaxslices\" : \"20\" , \"forecast_time\" : \"50\" , \"write_csv\" : True } MAPS The last step of the workflow is the visualization , namely the production of hazard maps. The required parameter is the average return period(s) expressed in years (it can be one or more values separated by comma). For example: Example { \"return_period\" : \"2500,25000,250000,1000000\" } HPC This section refers to the HPC cluster. In the present version, the entire execution of the workflow is performed on the same cluster, which is retrieved at runtime by a specific Python functionality. As a consequence, there is no need to fill out this section, unless the workflow is running on Leonardo @CINECA: in this case, the last 3 keys must be completed to be authorized to run by the job scheduler: Example { \"cluster\" : \" \" , \"username\" : \" \" , \"leonardo_account\" : \"test_account\" , \"leonardo_partition\" : \"boost_usr_prod\" , \"leonardo_quality\" : \"normal\" Warning Future versions will possibly allow to execute the workflow on a local cluster and then connect to a remote cluster for running the tsunami simulation, provided that values corresponding to the keys \"cluster\" and \"username\" are provided.","title":"The JSON input file"},{"location":"instructions/json_input/#the-json-input-file","text":"JSON (JavaScript Object Notation) is an open standard file format that uses human-readable text. The data are written in key/value pairs, separated by a colon(:). Different key/value pairs are separated by a comma(,). A template of the workflow_input.json is provided, where all the required keys are already defined and grouped as objects, i.e. collections of key/value pairs surrounded by a curly brace. Comments are disseminated all along the file for the sake of clarity, and featured by the key \"_comment\". Before running the workflow, the JSON input file should be appropriately filled, although some sections may be neglected, depending on which steps will be executed. In other words, not all the sections in the JSON file need to be filled from the beginning if the corresponding step is not expected to be executed, as each step can be turned on/off by setting True/False within the input file. On the other hand, each step needs the output of the previous steps. As explained in the installation instructions , the template must be copied and renamed. All the fields will now be described. SETTINGS This first section is mandatory from the very beginning. The (complete) paths for the working folder and the folders where the workflow and the topo-bathymetric grids are located must be defined. In addition, the script (with path) to activate the software environment must also be declared here, to run the steps on the computational nodes of the HPC cluster. Example { \"work_path\" : \"/work/user/ptha/test\" , \"workflow_dir\" : \"/home/user/cheese-ptha-master\" , \"bathy_dir\" : \"/work/user/ptha/bathy\" , \"env_file\" : \"/work/user/ptha/load_env.source\" } STEPS Each workflow step can be turned on/off by setting them as True/False , depending on whether they must be executed. It is worth noting that the output of each step is generally an input for the next one (except for STEP 4 , whose output is required by STEP 6 , while STEP 5 needs STEP 3 ). For example, only STEP 2 would be executed with the following setup, provided that STEP 1 has been previously done. Example { \"step1\" : False , \"step2\" : True , \"step3\" : False , \"step4\" : False , \"step5\" : False , \"step6\" : False , \"step7\" : False } POI_SELECTION Given the target site, the closest Points of Interest (POIs) are selected from the regional hazard model NEAMTHM18 in STEP 1 , and then used in the analysis. First of all, the geographic area of the target site must be declared, choosing among Mediterranean Sea (\"med\"), North-East Atlantic Ocean (\"nea\"), or Black Sea (\"black\"). It is worth noting that the workflow is automatized only for target sites overlooking the Mediterranean Sea, while some manual operations are required when dealing with the other regions. Then, in the Mediterranean Sea three different strategies are possible to select the POI(s), prioritized as follows: 1. if the high-resolution grid for the target site is available (and stored in the declared folder bathy_dir ) its domain can be used to draw a rectangular region containing the coastline of interest, and the POIs included in that region are selected; 2. if the coordinates of the target site are given, they are used to build the rectangle and search for the POIs within it; 3. if none of the previous options are provided, the label(s) of the POI(s) from NEAMTHM18 must be directly indicated, as in the following example: Example { \"domain\" : \"med\" , \"grdfile\" : \" \" , \"lonlat\" : \" \" , \"poinames\" : \"med07706,med07712\" } The last option is mandatory outside the Mediterranean Sea. REGIONAL MODEL The default regional model is NEAMTHM18 , and the related files are expected to be stored in a folder named \"TSUMAPS1.x\" depending on the version. Version 1.0 must be used for managing sites outside the Mediterranean Sea, while for the Mediterranean area an updated vesion 1.1 is available. The path in the example is referred to the cluster mercalli @INGV. Example { \"folderMain\" : \"/scratch/projects/cat/SPTHA/\" , \"project_name\" : \"TSUMAPS1.1\" } DISAGGREGATION When a disaggregation procedure is required to select relevant scenarios for the target site, some parameters must be defined such as: the choice between a disaggregation on MIH (Maximum Inundation Height) intervals (i) or thresholds (t); in case of intervals, the MIH interval(s), expressed in m, where the disaggregation must be executed (it can be a single interval, e.g. 1-4, or a sequence of intervals, e.g. 1-4,4-8); in case of thresholds, the MIH value(s), expressed in m, where the disaggregation must be executed (it can be one or more values separated by comma, e.g. 1,4); the desired percentage of hazard reproduction for each interval/threshold. For example, the following settings will select the scenarios by disaggregating the hazard to account for 90% of the total at 1 m and 4 m: Example { \"disaggregation\" : True , \"type\" : \"t\" , \"mih_int\" : \" \" , \"mih_val\" : \"1,4\" , \"threshold\" : \"90\" } SAMPLING An alternative/complementary method for scenario selection is the importance sampling . Warning The importance sampling has not been implemented yet, so it must be set as False { \"sampling\": False } SIMULATION SETUP Some parameters for the tsunami simulations must be defined, such as: the number of levels of nested grids; an array defining the refinement ratio of the nesting from the coarsest to the finest grid (it is worth noting that with the simulation code Tsunami-HySEA , a refinement ratio equal to a power of 2 is mandatory); the simulation length expressed in hours. For example, if the simulations are carried out for 4 hours on a grid setup made of an outer grid spanning a regional domain at a resolution of 320 m, and 4 local-scale telescopic grids with increasing resolution equal to 160, 80, 20, and 5 m respectively, this section would be: Example { \"nlevels\" : 5 , \"ref_ratio\" : [ 2 , 2 , 4 , 4 ], \"propagation\" : 4 } HAZARD The hazard aggregation is implemented both in Python and MATLAB, and the user can choose the preferred language, also depending on the software installed on the cluster where the workflow is running. Moreover, this step is parallelized on the domain, through a horizontal decomposition of the highest resolution grid in \"slices\", which are then processed in parallel. The number of slices (i.e. the number of parallel processes) is computed at runtime, but the maximum allowed number must be declared here, as a trade-off between parallelism and manageability. In addition, the desired forecast time window has to be set, i.e the time period for which the probability of exceedance must be computed (a typical value is 50 years). Finally, a logical variable allows to possibly write the hazard curves as a CSV table. For example: Example { \"haz_code\" : \"python\" , \"nmaxslices\" : \"20\" , \"forecast_time\" : \"50\" , \"write_csv\" : True } MAPS The last step of the workflow is the visualization , namely the production of hazard maps. The required parameter is the average return period(s) expressed in years (it can be one or more values separated by comma). For example: Example { \"return_period\" : \"2500,25000,250000,1000000\" } HPC This section refers to the HPC cluster. In the present version, the entire execution of the workflow is performed on the same cluster, which is retrieved at runtime by a specific Python functionality. As a consequence, there is no need to fill out this section, unless the workflow is running on Leonardo @CINECA: in this case, the last 3 keys must be completed to be authorized to run by the job scheduler: Example { \"cluster\" : \" \" , \"username\" : \" \" , \"leonardo_account\" : \"test_account\" , \"leonardo_partition\" : \"boost_usr_prod\" , \"leonardo_quality\" : \"normal\" Warning Future versions will possibly allow to execute the workflow on a local cluster and then connect to a remote cluster for running the tsunami simulation, provided that values corresponding to the keys \"cluster\" and \"username\" are provided.","title":"The JSON input file"},{"location":"instructions/requirements/","text":"Requirements The following software is required: Bash shell Preferred package manager (suggested: Spack ) Python3.x Tsunami-HySEA code (MC version) Optional software: MATLAB The required input data are: input file workflow_input.json in JSON format, containing all the settings and parameters: a template for such file is provided with the workflow (see installation instructions ); input file load_env.source , containing commands to activate the software virtual environment: a template for such file is provided with the workflow (see installation instructions ); regional hazard model for locations, mechanisms, sizes, and probabilities of occurrence of earthquake sources (here NEAMTHM18 ; v1.0 for target sites in the North-East Atlantic or Black Sea areas; v1.1 for target sites in the Mediterranean region); data are already available on the HPC clusters foreseen by the workflow; local telescopic grids + outer grid level at regional scale: a set of files, suitably formatted for the T-HySEA code (netCDF format with extension .grd), named SITENAME_grid0.grd , SITENAME_grid1.grd etc., from the coarsest to the finest resolution ( more details ). All these files must be located in the folder specified in the JSON input file ( /path-to-grids/ ) and must be available before running the tsunami simulations ( STEP 5 ); the finest grid, if available, can also be used to retrieve the Point of Interest (POI) closest to the target site from NEAMTHM18 , but other strategies are possible ( STEP 1 ).","title":"Requirements"},{"location":"instructions/requirements/#requirements","text":"The following software is required: Bash shell Preferred package manager (suggested: Spack ) Python3.x Tsunami-HySEA code (MC version) Optional software: MATLAB The required input data are: input file workflow_input.json in JSON format, containing all the settings and parameters: a template for such file is provided with the workflow (see installation instructions ); input file load_env.source , containing commands to activate the software virtual environment: a template for such file is provided with the workflow (see installation instructions ); regional hazard model for locations, mechanisms, sizes, and probabilities of occurrence of earthquake sources (here NEAMTHM18 ; v1.0 for target sites in the North-East Atlantic or Black Sea areas; v1.1 for target sites in the Mediterranean region); data are already available on the HPC clusters foreseen by the workflow; local telescopic grids + outer grid level at regional scale: a set of files, suitably formatted for the T-HySEA code (netCDF format with extension .grd), named SITENAME_grid0.grd , SITENAME_grid1.grd etc., from the coarsest to the finest resolution ( more details ). All these files must be located in the folder specified in the JSON input file ( /path-to-grids/ ) and must be available before running the tsunami simulations ( STEP 5 ); the finest grid, if available, can also be used to retrieve the Point of Interest (POI) closest to the target site from NEAMTHM18 , but other strategies are possible ( STEP 1 ).","title":"Requirements"},{"location":"instructions/telescopic_grids/","text":"How to build telescopic nested grids The Tsunami-HySEA code for the numerical simulation of tsunami generation, propagation, and inundation exploits the telescopic nested grid algorithm to have progressively higher resolution as far as the target site is approached. This allows to save computational time and resources, obtaining a detailed local modeling of the inundation without the need of using the same resolution in the whole simulation domain. The typical setup is made by an offshore bathymetric grid at regional scale, having a resolution of hundreds of meters, and some local grids, up to 5 \u2013 10 m resolution, extending a few kilometers to a few tens of kilometers in North-South and East-West directions. [...to be continued]","title":"How to build telescopic nested grids"},{"location":"instructions/telescopic_grids/#how-to-build-telescopic-nested-grids","text":"The Tsunami-HySEA code for the numerical simulation of tsunami generation, propagation, and inundation exploits the telescopic nested grid algorithm to have progressively higher resolution as far as the target site is approached. This allows to save computational time and resources, obtaining a detailed local modeling of the inundation without the need of using the same resolution in the whole simulation domain. The typical setup is made by an offshore bathymetric grid at regional scale, having a resolution of hundreds of meters, and some local grids, up to 5 \u2013 10 m resolution, extending a few kilometers to a few tens of kilometers in North-South and East-West directions. [...to be continued]","title":"How to build telescopic nested grids"},{"location":"instructions/usage/","text":"Usage The workflow is organized as a stepwise procedure, where the steps can be turned on/off through Boolean flags. It is configured by one input file in JSON format, named workflow_input.json , which must be properly filled by the user before starting. The execution is managed by the Python script workflow_main.py in the main folder. All the other scripts are in the folder py , while bash scripts for submitting jobs to the computational nodes are in the folder sh . To run, activate the software environment as explained here , or using a different package manager, and type (assuming the JSON input file is in the working folder): python /path-to-sofware/cheese-ptha-master/workflow_main.py --sitename SITENAME --input_workflow /path-to-wdir/workflow_input.json The steps marked as True in the JSON input file will be executed. Of course, if one single step is required, the previous ones are assumed to be already executed. In production mode, it is strongly recommended to run the workflow preventing the process from receiving the SIGHUP (Signal Hang UP) signal upon closing or exiting the terminal: nohup pyhton /path-to-software/cheese-ptha-master/workflow_main.py --sitename SITENAME --input_workflow /path-to-wdir/workflow_input.json & A new folder, called SITENAME, is created within the working folder, and all the outputs will be directed there.","title":"Usage"},{"location":"instructions/usage/#usage","text":"The workflow is organized as a stepwise procedure, where the steps can be turned on/off through Boolean flags. It is configured by one input file in JSON format, named workflow_input.json , which must be properly filled by the user before starting. The execution is managed by the Python script workflow_main.py in the main folder. All the other scripts are in the folder py , while bash scripts for submitting jobs to the computational nodes are in the folder sh . To run, activate the software environment as explained here , or using a different package manager, and type (assuming the JSON input file is in the working folder): python /path-to-sofware/cheese-ptha-master/workflow_main.py --sitename SITENAME --input_workflow /path-to-wdir/workflow_input.json The steps marked as True in the JSON input file will be executed. Of course, if one single step is required, the previous ones are assumed to be already executed. In production mode, it is strongly recommended to run the workflow preventing the process from receiving the SIGHUP (Signal Hang UP) signal upon closing or exiting the terminal: nohup pyhton /path-to-software/cheese-ptha-master/workflow_main.py --sitename SITENAME --input_workflow /path-to-wdir/workflow_input.json & A new folder, called SITENAME, is created within the working folder, and all the outputs will be directed there.","title":"Usage"},{"location":"other/disaggregation/","text":"The disaggregation procedure This procedure finds, among all of the scenarios retrieved from STEP 2 , the most relevant ones for the target site, i.e. the scenarios that contribute most significantly to the tsunami hazard at each POI, within predefined MIH (Maximum Inundation Heights) intervals or thresholds and fixing the desired percentage of hazard reproduction. It is based on the mean model of the epistemic uncertainty. For example, we may want a list of those scenarios accounting for 95% of the total hazard within the 1-4 m MIH range, or for MIH > 1 m (more intervals or more thresholds are allowed). For each selected POI, the algorithm first ranks all the scenarios contributing to a given intensity, or intensity interval, according to their relative importance for the site of interest, measured as their relative contribution to the local offshore hazard curve in terms of mean annual rates. Then, the scenarios are taken up to the desired \u201clevel of accuracy\u201d to which the local hazard should be approximated, defined in terms of the resemblance of the hazard curve calculated at the POI using the selected scenarios with respect to the original one. A separate output folder is created, named DISAGGREGATION_POIname , for each POI. Then, the lists are merged removing duplicates, and the net list of scenarios is saved in a new folder DISAGGREGATION .","title":"The disaggregation procedure"},{"location":"other/disaggregation/#the-disaggregation-procedure","text":"This procedure finds, among all of the scenarios retrieved from STEP 2 , the most relevant ones for the target site, i.e. the scenarios that contribute most significantly to the tsunami hazard at each POI, within predefined MIH (Maximum Inundation Heights) intervals or thresholds and fixing the desired percentage of hazard reproduction. It is based on the mean model of the epistemic uncertainty. For example, we may want a list of those scenarios accounting for 95% of the total hazard within the 1-4 m MIH range, or for MIH > 1 m (more intervals or more thresholds are allowed). For each selected POI, the algorithm first ranks all the scenarios contributing to a given intensity, or intensity interval, according to their relative importance for the site of interest, measured as their relative contribution to the local offshore hazard curve in terms of mean annual rates. Then, the scenarios are taken up to the desired \u201clevel of accuracy\u201d to which the local hazard should be approximated, defined in terms of the resemblance of the hazard curve calculated at the POI using the selected scenarios with respect to the original one. A separate output folder is created, named DISAGGREGATION_POIname , for each POI. Then, the lists are merged removing duplicates, and the net list of scenarios is saved in a new folder DISAGGREGATION .","title":"The disaggregation procedure"},{"location":"other/sampling/","text":"The importance sampling procedure Warning This page is under constrution","title":"The importance sampling procedure"},{"location":"other/sampling/#the-importance-sampling-procedure","text":"Warning This page is under constrution","title":"The importance sampling procedure"},{"location":"spack/env_spack/","text":"How to use Spack These instructions describe how to set up a Spack environment including Python3.x, py-pip and all the packages required to run the workflow. Info To know more about Spack: Spack documentation . Here you can find some common Spack commands. Follow the instructions for: Leonardo @CINECA Mercalli @INGV","title":"How to use Spack"},{"location":"spack/env_spack/#how-to-use-spack","text":"These instructions describe how to set up a Spack environment including Python3.x, py-pip and all the packages required to run the workflow. Info To know more about Spack: Spack documentation . Here you can find some common Spack commands. Follow the instructions for: Leonardo @CINECA Mercalli @INGV","title":"How to use Spack"},{"location":"spack/leonardo/","text":"Instructions for Leonardo On leonardo @CINECA, Spack is already installed as a module, and the user just has to load it and then create and configure the environment: in your home directory, load Spack: module load spack/0.19.1-d71 create and activate the environment with the following commands: spack env create -d spack_env_name spack env activate [-p] ~/spack_env_name Note that the -p is just a flag to visualize the environment is active. add Python and py-pip to the environment and install them spack add python py-pip spack -d install In this way, the (last) preferred version of Python will be installed. You can do spack find to see which packages have been installed. Tip In case you want to install a specific Python version, you can search and choose the preferred version through the commands spack info python spack add python@version install Python packages needed to run the WF in the environment pip install -r /path-to-software/cheese-ptha-master/requirements.txt The file requirements.txt is provided with the workflow. Warning You could get the error ModuleNotFoundError: No module named 'pip' . If this is the case, you have to load py-pip before installing the Python packages spack load py-pip The environment is ready. For any new session, of course, it must be activated by the commands module load spack/0.19.1-d71 spack env activate [-p] ~/spack_env_name The same two lines should be inserted within the file load_env.source .","title":"Instructions for Leonardo"},{"location":"spack/leonardo/#instructions-for-leonardo","text":"On leonardo @CINECA, Spack is already installed as a module, and the user just has to load it and then create and configure the environment: in your home directory, load Spack: module load spack/0.19.1-d71 create and activate the environment with the following commands: spack env create -d spack_env_name spack env activate [-p] ~/spack_env_name Note that the -p is just a flag to visualize the environment is active. add Python and py-pip to the environment and install them spack add python py-pip spack -d install In this way, the (last) preferred version of Python will be installed. You can do spack find to see which packages have been installed. Tip In case you want to install a specific Python version, you can search and choose the preferred version through the commands spack info python spack add python@version install Python packages needed to run the WF in the environment pip install -r /path-to-software/cheese-ptha-master/requirements.txt The file requirements.txt is provided with the workflow. Warning You could get the error ModuleNotFoundError: No module named 'pip' . If this is the case, you have to load py-pip before installing the Python packages spack load py-pip The environment is ready. For any new session, of course, it must be activated by the commands module load spack/0.19.1-d71 spack env activate [-p] ~/spack_env_name The same two lines should be inserted within the file load_env.source .","title":"Instructions for Leonardo"},{"location":"spack/mercalli/","text":"Instructions for Mercalli On mercalli @INGV Spack is not installed, and the user has to do it locally in his/her own area. Moreover, since Python 2.7 is in use by default, while a version >3.6 is required by Spack, a preliminary step must be executed to force Spack to use Python >3.6, in order to avoid getting this error message Spack requires Python 3.6 or higher You are running spack with Python 2.7.5 . add the following line to your ~/.bashrc file: export SPACK_PYTHON=/soft/centos7/anaconda/2023.03/bin/python in your home directory, run the following commands to download and configure Spack: git clone -c feature.manyFiles=true https://github.com/spack/spack.git . spack/share/spack/setup-env.sh Tip To avoid running the same command every time you open a new terminal, you might want to add this line to your ~/.bashrc file: . ~/spack/share/spack/setup-env.sh create and activate the environment with the following commands: spack env create -d spack_env_name spack env activate [-p] ~/spack_env_name Note that the -d option will create the environment in the current (home) folder, while omitting it will result in creating the environment in the default folder ( ~/spack/var/spack/environments/ ). The -p is just a flag to visualize the environment is active. add Python and py-pip to the environment and install them spack add python py-pip spack -d install In this way, the (last) preferred version of Python will be installed. You can do spack find to see which packages have been installed. Tip In case you want to install a specific Python version, you can search and choose the preferred version through the commands spack info python spack add python@version install Python packages needed to run the WF in the environment pip install -r /path-to-software/cheese-ptha-master/requirements.txt The file requirements.txt is provided with the workflow. Warning You could get the error ModuleNotFoundError: No module named 'pip' . If this is the case, you have to load py-pip before installing the Python packages spack load py-pip The environment is ready. For any new session, of course, it must be activated by the command spack env activate [-p] ~/spack_env_name The same line should be inserted within the file load_env.source .","title":"Instructions for Mercalli"},{"location":"spack/mercalli/#instructions-for-mercalli","text":"On mercalli @INGV Spack is not installed, and the user has to do it locally in his/her own area. Moreover, since Python 2.7 is in use by default, while a version >3.6 is required by Spack, a preliminary step must be executed to force Spack to use Python >3.6, in order to avoid getting this error message Spack requires Python 3.6 or higher You are running spack with Python 2.7.5 . add the following line to your ~/.bashrc file: export SPACK_PYTHON=/soft/centos7/anaconda/2023.03/bin/python in your home directory, run the following commands to download and configure Spack: git clone -c feature.manyFiles=true https://github.com/spack/spack.git . spack/share/spack/setup-env.sh Tip To avoid running the same command every time you open a new terminal, you might want to add this line to your ~/.bashrc file: . ~/spack/share/spack/setup-env.sh create and activate the environment with the following commands: spack env create -d spack_env_name spack env activate [-p] ~/spack_env_name Note that the -d option will create the environment in the current (home) folder, while omitting it will result in creating the environment in the default folder ( ~/spack/var/spack/environments/ ). The -p is just a flag to visualize the environment is active. add Python and py-pip to the environment and install them spack add python py-pip spack -d install In this way, the (last) preferred version of Python will be installed. You can do spack find to see which packages have been installed. Tip In case you want to install a specific Python version, you can search and choose the preferred version through the commands spack info python spack add python@version install Python packages needed to run the WF in the environment pip install -r /path-to-software/cheese-ptha-master/requirements.txt The file requirements.txt is provided with the workflow. Warning You could get the error ModuleNotFoundError: No module named 'pip' . If this is the case, you have to load py-pip before installing the Python packages spack load py-pip The environment is ready. For any new session, of course, it must be activated by the command spack env activate [-p] ~/spack_env_name The same line should be inserted within the file load_env.source .","title":"Instructions for Mercalli"},{"location":"spack/useful_commands/","text":"Useful Spack commands Below is a short list of some useful commands. For the complete list of Spack commands check the documentation Command Comment spack env create -d . To create a new environment in the current folder spack env activate -p . To activate an environment in the current folder spack env deactivate To deactivate the environment spack list packagename To see available packages that contain \"packagename\" on spack spack info packagename To see which versions of a specific package are available spack add packagename To add a new package (then do spack concretize and spack install ) spack add packagename@x.x.x To add a new package, where x.x.x defines a specific version spack install To install packages added to the environment spack load installed-package To load a previously installed package in the environment* spack find To see what packages are installed in the environment spack find --loaded To see what packages are loaded in the environment spack find --paths To see where packages are installed within the spack environment spack repo add /path/ To add a new repository (in addition to the default one from spack)","title":"Useful Spack commands"},{"location":"spack/useful_commands/#useful-spack-commands","text":"Below is a short list of some useful commands. For the complete list of Spack commands check the documentation Command Comment spack env create -d . To create a new environment in the current folder spack env activate -p . To activate an environment in the current folder spack env deactivate To deactivate the environment spack list packagename To see available packages that contain \"packagename\" on spack spack info packagename To see which versions of a specific package are available spack add packagename To add a new package (then do spack concretize and spack install ) spack add packagename@x.x.x To add a new package, where x.x.x defines a specific version spack install To install packages added to the environment spack load installed-package To load a previously installed package in the environment* spack find To see what packages are installed in the environment spack find --loaded To see what packages are loaded in the environment spack find --paths To see where packages are installed within the spack environment spack repo add /path/ To add a new repository (in addition to the default one from spack)","title":"Useful Spack commands"},{"location":"steps/overview/","text":"General overview STEP 1 \u2192 Selection of the Point(s) of Interest (POI) from NEAMTHM18 ), using the extension of the local grid or the coordinates of the site of interest or directly the labels of the desired POIs, if known. STEP 2 \u2192 Scenario Dumping: all of the earthquake scenarios and corresponding metadata that contribute to the tsunami hazard at the target site are extracted from the NEAMTHM18 database. STEP 3 \u2192 Scenario Selection: a number of relevant scenarios significantly contributing to the tsunami hazard at the target site is selected. Presently, the implemented procedure is a disaggregation from the total hazard, based on the mean model of the epistemic uncertainty. In the future, other approaches will be implemented, such as the importance sampling . STEP 4 \u2192 Scenario Annual Rates: the annual rates of occurrence, including the whole epistemic uncertainty, are retrieved for all of the scenarios obtained from the sampling procedure. STEP 5 \u2192 Tsunami Simulations: high-resolution numerical simulations of the tsunami generation, propagation and inundation on telescopic nested grids are set up and executed for each scenario, using the Tsunami-HySEA numerical code. STEP 6 \u2192 Hazard aggregation: the output from each simulation is combined with the scenario rates to calculate the hazard curves on a predefined set of MIH (Maximum Inundation Height) thresholds. STEP 7 \u2192 Visualization: hazard maps for selected average return periods (ARPs) are produced for visualization.","title":"General overview"},{"location":"steps/overview/#general-overview","text":"STEP 1 \u2192 Selection of the Point(s) of Interest (POI) from NEAMTHM18 ), using the extension of the local grid or the coordinates of the site of interest or directly the labels of the desired POIs, if known. STEP 2 \u2192 Scenario Dumping: all of the earthquake scenarios and corresponding metadata that contribute to the tsunami hazard at the target site are extracted from the NEAMTHM18 database. STEP 3 \u2192 Scenario Selection: a number of relevant scenarios significantly contributing to the tsunami hazard at the target site is selected. Presently, the implemented procedure is a disaggregation from the total hazard, based on the mean model of the epistemic uncertainty. In the future, other approaches will be implemented, such as the importance sampling . STEP 4 \u2192 Scenario Annual Rates: the annual rates of occurrence, including the whole epistemic uncertainty, are retrieved for all of the scenarios obtained from the sampling procedure. STEP 5 \u2192 Tsunami Simulations: high-resolution numerical simulations of the tsunami generation, propagation and inundation on telescopic nested grids are set up and executed for each scenario, using the Tsunami-HySEA numerical code. STEP 6 \u2192 Hazard aggregation: the output from each simulation is combined with the scenario rates to calculate the hazard curves on a predefined set of MIH (Maximum Inundation Height) thresholds. STEP 7 \u2192 Visualization: hazard maps for selected average return periods (ARPs) are produced for visualization.","title":"General overview"},{"location":"steps/step1/","text":"STEP 1: POI(s) selection This step aims to select, from NEAMTHM18 database, the POI(s) which fall within a rectangular area containing the coastlines around the target site. Such rectangular area can be built in two ways: if the high-resolution inner grid is available and declared within the JSON input file , the grid domain is used to draw a rectangle whose sides are then expanded by \\(X\\) kilometers perpendicularly ( \\(X=10\\) km by default); if the key \"grdfile\" in the input file is empty while the coordinates of the target site are provided, the first rectangle is drawn by moving \\(\\pm X^\\circ\\) from the site both vertically and horizontally ( \\(X = 0.1^\\circ\\) by default) and then its sides are extended as before; if both the keys \"grdfile\" and \"lonlat\" are empty, the labels of the POIs are expected to be given by the user in the following field \"poinames\". The outputs of this step are text files with labels (_POI_name.txt) and coordinates (_ts.dat) of the selected POI(s). The last one is formatted to make the HySEA code save the time series at the POIs when running the tsunami simulations ( Step 5 ).","title":"STEP 1: POI(s) selection"},{"location":"steps/step1/#step-1-pois-selection","text":"This step aims to select, from NEAMTHM18 database, the POI(s) which fall within a rectangular area containing the coastlines around the target site. Such rectangular area can be built in two ways: if the high-resolution inner grid is available and declared within the JSON input file , the grid domain is used to draw a rectangle whose sides are then expanded by \\(X\\) kilometers perpendicularly ( \\(X=10\\) km by default); if the key \"grdfile\" in the input file is empty while the coordinates of the target site are provided, the first rectangle is drawn by moving \\(\\pm X^\\circ\\) from the site both vertically and horizontally ( \\(X = 0.1^\\circ\\) by default) and then its sides are extended as before; if both the keys \"grdfile\" and \"lonlat\" are empty, the labels of the POIs are expected to be given by the user in the following field \"poinames\". The outputs of this step are text files with labels (_POI_name.txt) and coordinates (_ts.dat) of the selected POI(s). The last one is formatted to make the HySEA code save the time series at the POIs when running the tsunami simulations ( Step 5 ).","title":"STEP 1: POI(s) selection"},{"location":"steps/step2/","text":"STEP 2: Scenario dumping This step executes a first screening of the potential scenarios, by extracting from the NEAMTHM18 database all of the earthquake scenarios (and corresponding metadata) which contribute to the tsunami hazard at the target site, i.e. at each selected POI. This step is automatized when using NEAMTHM18 v. 1.1 (i.e. when the target site is located in the Mediterranean area), and is parallelized on the regions of the tectonic regionalization: through a job array, one process for each region (and for each POI) is executed on the computational nodes of the HCP cluster. If scenarios affecting the POI are found in the region, separate output files are possibly produced for each seismicity type. For each POI, a separate output folder is created named DUMPING_poiname . In case NEAMTHM18 v. 1.0 must be used (i.e. outside the Mediterranean), a specific code must be manually executed, producing a single output file. The output of this step serves as input for the following STEP 3 .","title":"STEP 2: Scenario dumping"},{"location":"steps/step2/#step-2-scenario-dumping","text":"This step executes a first screening of the potential scenarios, by extracting from the NEAMTHM18 database all of the earthquake scenarios (and corresponding metadata) which contribute to the tsunami hazard at the target site, i.e. at each selected POI. This step is automatized when using NEAMTHM18 v. 1.1 (i.e. when the target site is located in the Mediterranean area), and is parallelized on the regions of the tectonic regionalization: through a job array, one process for each region (and for each POI) is executed on the computational nodes of the HCP cluster. If scenarios affecting the POI are found in the region, separate output files are possibly produced for each seismicity type. For each POI, a separate output folder is created named DUMPING_poiname . In case NEAMTHM18 v. 1.0 must be used (i.e. outside the Mediterranean), a specific code must be manually executed, producing a single output file. The output of this step serves as input for the following STEP 3 .","title":"STEP 2: Scenario dumping"},{"location":"steps/step3/","text":"STEP 3: Scenario selection To reduce the number of scenarios retrieved from the dumping procedure, only considering the most relevant ones, and save computational time and resources, different strategies can be used, as alternatives to each other or in cascade: Disaggregation Importance Sampling - (not implemented yet)","title":"STEP 3: Scenario selection"},{"location":"steps/step3/#step-3-scenario-selection","text":"To reduce the number of scenarios retrieved from the dumping procedure, only considering the most relevant ones, and save computational time and resources, different strategies can be used, as alternatives to each other or in cascade: Disaggregation Importance Sampling - (not implemented yet)","title":"STEP 3: Scenario selection"},{"location":"steps/step4/","text":"STEP 4: Scenario rates This step interrogates the NEAMTHM18 database to retrieve the probabilities of occurrence of each of the scenarios selected by STEP 3 , including the whole epistemic uncertainty. In other words, for any given earthquake scenario, we obtain a distribution of the mean annual rates of occurrence from 1000 alternative models. This step is automatized when using NEAMTHM18 v. 1.1 (i.e. when the target site is located in the Mediterranean area), and is parallelized on the regions of the tectonic regionalization where the selected scenarios are located: through a job array, one process for each involved region is executed on the computational nodes of the HCP cluster. Separate output files for each region are produced for each seismicity type, and merged at the end. In case NEAMTHM18 v. 1.0 must be used (i.e. outside the Mediterranean), a specific code must be manually executed, producing a single output file for each seismicity type. The final output is saved in the output folder RATES and will be used by STEP 6 .","title":"STEP 4: Scenario rates"},{"location":"steps/step4/#step-4-scenario-rates","text":"This step interrogates the NEAMTHM18 database to retrieve the probabilities of occurrence of each of the scenarios selected by STEP 3 , including the whole epistemic uncertainty. In other words, for any given earthquake scenario, we obtain a distribution of the mean annual rates of occurrence from 1000 alternative models. This step is automatized when using NEAMTHM18 v. 1.1 (i.e. when the target site is located in the Mediterranean area), and is parallelized on the regions of the tectonic regionalization where the selected scenarios are located: through a job array, one process for each involved region is executed on the computational nodes of the HCP cluster. Separate output files for each region are produced for each seismicity type, and merged at the end. In case NEAMTHM18 v. 1.0 must be used (i.e. outside the Mediterranean), a specific code must be manually executed, producing a single output file for each seismicity type. The final output is saved in the output folder RATES and will be used by STEP 6 .","title":"STEP 4: Scenario rates"},{"location":"steps/step5/","text":"STEP 5: Tsunami simulations","title":"STEP 5: Tsunami simulations"},{"location":"steps/step5/#step-5-tsunami-simulations","text":"","title":"STEP 5: Tsunami simulations"},{"location":"steps/step6/","text":"STEP 6: Hazard aggregation This step generates a new folder called HAZARD in the working directory and calculates the hazard curves at each point of the local finest grid for different percentiles of the epistemic uncertainty, by combining the output of each simulation from STEP 5 with the scenario rates retrieved in STEP 4 . To speed up the process, this step is parallelized on the domain, through a horizontal decomposition (along the y direction) of the grid in \"slices\", which are then processed in parallel, so that each process manages only the points within one slice. The number of slices (i.e. the number of parallel processes) is computed at runtime by the workflow, but the maximum allowed number must be declared in the JSON input file , as a trade-off between parallelism and manageability. Finally, all the results are recombined. The code is available both in Python and MATLAB, and the user can choose the preferred language in the JSON input file , also depending on the software available on the cluster in use.","title":"STEP 6: Hazard aggregation"},{"location":"steps/step6/#step-6-hazard-aggregation","text":"This step generates a new folder called HAZARD in the working directory and calculates the hazard curves at each point of the local finest grid for different percentiles of the epistemic uncertainty, by combining the output of each simulation from STEP 5 with the scenario rates retrieved in STEP 4 . To speed up the process, this step is parallelized on the domain, through a horizontal decomposition (along the y direction) of the grid in \"slices\", which are then processed in parallel, so that each process manages only the points within one slice. The number of slices (i.e. the number of parallel processes) is computed at runtime by the workflow, but the maximum allowed number must be declared in the JSON input file , as a trade-off between parallelism and manageability. Finally, all the results are recombined. The code is available both in Python and MATLAB, and the user can choose the preferred language in the JSON input file , also depending on the software available on the cluster in use.","title":"STEP 6: Hazard aggregation"},{"location":"steps/step7/","text":"STEP 7: Hazard visualization","title":"STEP 7: Hazard visualization"},{"location":"steps/step7/#step-7-hazard-visualization","text":"","title":"STEP 7: Hazard visualization"}]}